{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e267051-d69f-4f8a-ba5d-55fcb9efb555",
   "metadata": {},
   "source": [
    "# A tutorial on kernel compatibility in Python: kernel-target and task-model alignments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I stumbled in this article:\n",
    "\n",
    "> Canatar, Abdulkadir, Blake Bordelon, and Cengiz Pehlevan. \"Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks.\" Nature communications 12.1 (2021): 1-12.\n",
    "\n",
    "The purpose of this blog post is illustrating this wonderful approach and writing a little code to give a warm start. \n",
    "\n",
    "We setup the Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "84e703cb-dd9a-4161-a189-c879e499553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra library\n",
    "import numpy as np\n",
    "# advanced scientific programming library\n",
    "import scipy\n",
    "from scipy.linalg import eigh\n",
    "from scipy.spatial.distance import cdist\n",
    "# machine learning utilities, e.g. loading some datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# for reproducibility purposes, fix a random seed at the beginning\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59b65d-b6de-48b9-b4df-af3c714723fa",
   "metadata": {},
   "source": [
    "## Problem setup\n",
    "\n",
    "Consider a set of empirical data $x^{(1)}, ..., x^{(M)} \\in \\mathcal{X} \\subseteq \\mathbb{R}^P$, drawn i.i.d. from an unknown probability density function $p$, and its corresponding labels $y^{(i)} \\in \\mathbb{R}, y^{(i)} = f(x^{(i)}) + \\epsilon^{(i)}$ generated from a target function $f$ plus an addictive white noise (drawn from a normal distribution of zero mean and variance $\\sigma^2$).\n",
    "\n",
    "For this tutorial we consider the California housing dataset, composed of $M = 20640$ items having $P = 8$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a99a21e7-473e-41a4-b222-1095fb4e6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "X_dataset = dataset['data']    # np.array of size (20640, 8)\n",
    "y_dataset = dataset['target']  # np.array of size (20640,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b105c6-a332-483b-94f7-5dc5d44ca624",
   "metadata": {},
   "source": [
    "There are too many samples, we can reduce it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "76058f06-d79a-4155-9e7e-7af0750d94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50\n",
    "X = X_dataset[:M]\n",
    "y = y_dataset[:M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d88141-4584-42ec-9d05-ecfa9efdd043",
   "metadata": {},
   "source": [
    "## Kernel models \n",
    "\n",
    "A positive definite function $\\kappa : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ define on a non-empty set $\\mathcal{X}$ is called a kernel. A kernel function generalizes the notion of similarity between samples by mapping the points from the original feature space $\\mathcal{X}$ into a different (usually richer, higher dimensional) Hilbert space $\\mathcal{H}$ through a feature map $\\phi : \\mathcal{X} \\to \\mathcal{H}$ and then taking their inner product in $\\mathcal{H}$. It holds that $\\kappa(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_\\mathcal{H}$. \n",
    "\n",
    "The Gram matrix $K = [\\kappa(x^{(i)}, x^{(j)})]_{i,j=1}^M$ is the matrix of pairwise kernel similarities. Given a p.d. kernel $\\kappa$ and two sets $X_1 \\in \\mathbb{R}^{M\\times P}, X_2 \\in \\mathbb{R}^{M'\\times P}$ we can use the 'scipy' function 'cdist' to construct the $M \\times M'$ Gram matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1e447ead-1ccc-48e1-9e88-32625a0cd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kernel(kappa, X1, X2):\n",
    "    return cdist(X1, X2, metric=kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38440ed-c3b0-40bb-9f7c-ce05a26bcddc",
   "metadata": {},
   "source": [
    "### Kernel eigendecomposition\n",
    "\n",
    "We can analyze several properties of the kernel, such as the one arising from the integral operator\n",
    "$$(T_\\kappa)(f)(x) = \\int_\\mathcal{X} k(x, x') f(x') p(x') dx'$$\n",
    "We can calculate the set of (possibly infinite) real, non-negative eigenvalues $\\{ \\eta_i \\}$ and orthogonal eigenfunctions $\\{ \\phi_i \\}$, leading to (Mercer theorem) an eigendecomposition of $\\kappa$:\n",
    "$$\\kappa(x, x') = \\sum_{i=1}^\\infty \\eta_i \\phi_i(x) \\phi_i(x').$$\n",
    "\n",
    "The eigenvalue distribution can be approximated by one of the Gram matrix $k$ as $M \\to +\\infty$. The Hermitianity of the matrix $K$ guarantees that $K = \\Phi \\Lambda \\Phi^\\dagger$ where $\\Phi = [\\phi(x^{(i)})]_{i=1}^M$ is an unitary matrix and $\\Lambda = \\text{diag}(\\lambda_0, ..., \\lambda_{M-1})$ is a diagonal of eigenvalues sorted in descending order. We can use the following function to apply the decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "23a5e35f-8ed2-4145-9bf2-09733ffbbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_kernel(K, eigenvalue_descending_order=True, eigenvalue_removal_threshold=1e-12):\n",
    "    \"\"\"\n",
    "    Decompose the kernel matrix K in its eigenvalues Λ and eigenvectors Φ\n",
    "    :param K: kernel matrix, real and symmetric\n",
    "    :param eigenvalue_descending_order: if True, the biggest eigenvalue is the first one\n",
    "    :return: Lambda vector (n elements) and Phi matrix (N*N matrix)\n",
    "    \"\"\"\n",
    "    Lambda, Phi = eigh(K)\n",
    "\n",
    "    # set the desired order for the eigenvalues\n",
    "    if eigenvalue_descending_order:\n",
    "        Lambda = Lambda[::-1]\n",
    "        Phi = Phi[:, ::-1]\n",
    "\n",
    "    # kernel matrix is positive definite, any (small) negative eigenvalue is effectively a numerical error\n",
    "    Lambda[Lambda < 0] = 0\n",
    "\n",
    "    # remove smallest positive eigenvalues, as they are useless\n",
    "    Lambda[Lambda < eigenvalue_removal_threshold] = 0\n",
    "\n",
    "    return Lambda, Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc589ab-e410-46bc-ae7f-aa154b946684",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regressor\n",
    "\n",
    "We use the Kernel Ridge Regressor which is the function:\n",
    "$$f^*(x) = \\arg\\min_{f \\in H} \\sum_{i=1}^M (f(x^{(i)}) - y^{(i)})^2 + \\lambda \\lVert f \\rVert$$\n",
    "whose function can be expressed as\n",
    "$$f^*(x) = \\sum_{i=1}^M \\alpha^{(i)} \\kappa(x, x^{(i)})$$\n",
    "and \n",
    "$$\\alpha = (K + \\lambda I)^{-1} y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ec558320-cfc7-435e-8713-d7eee720a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alpha_krr(kappa, the_X, the_y, lambda_coef):\n",
    "    the_K = build_kernel(kappa, the_X, the_X)\n",
    "    # solve with Least Square (needed if we incur in a singular matrix)\n",
    "    the_alpha = scipy.linalg.lstsq(the_K + lambda_coef * np.eye(the_K.shape[0]), y.ravel())[0]\n",
    "    # more effient solve\n",
    "    # the_alpha = scipy.linalg.solve(the_K + lambda_coef * np.eye(the_K.shape[0]), y)\n",
    "    return the_alpha\n",
    "\n",
    "def build_krr(kappa, the_X, the_y, lambda_coef):\n",
    "    the_alpha = build_alpha_krr(kappa, the_X, the_y, lambda_coef)\n",
    "    def krr(the_X_test):\n",
    "        K_test = build_kernel(kappa, the_X_test, the_X)\n",
    "        return K_test @ the_alpha\n",
    "    return krr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15caa4fe-3ca8-498b-a2ce-d0618b573a66",
   "metadata": {},
   "source": [
    "We can double check the correctness of the implementation by comparing it with sklearn KRR's one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "9830c9e8-138b-494d-8a13-4b8df9d4cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg difference in alpha coefficients: 0.00004599\n",
      "Avg difference in y predicted: 0.00000067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "sklearn_krr = KernelRidge(kernel=np.inner, alpha=0.001)\n",
    "sklearn_krr.fit(X, y)\n",
    "\n",
    "# checking alpha coefficient matches\n",
    "alpha_krr = sklearn_krr.dual_coef_\n",
    "alpha_krr_sk = build_alpha_krr(np.inner, X, y, 0.001)\n",
    "print(f\"Avg difference in alpha coefficients: {np.linalg.norm(alpha_krr - alpha_krr_sk) / len(alpha_krr):0.8f}\")\n",
    "\n",
    "krr = build_krr(np.inner, X, y, 0.001)\n",
    "y_pred = krr(X_dataset[20:25, :])\n",
    "y_pred_sk = sklearn_krr.predict(X_dataset[20:25, :])\n",
    "print(f\"Avg difference in y predicted: {np.linalg.norm(y_pred - y_pred_sk) / len(y_pred_sk):0.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9d5bc-98c6-428a-b296-57a00e227e9f",
   "metadata": {},
   "source": [
    "### Kernel compatibility\n",
    "\n",
    "#### Kernel-Target alignment\n",
    "\n",
    "The kernel aligment has been proposed in [1] as a similarity measure between kernels:\n",
    "$$A(K_1, K_2) = \\frac{\\langle K_1, K_2 \\rangle}{\\sqrt{\\langle K_1, K_1 \\rangle \\langle K_2, K_2 \\rangle}} \\in [-1, +1]$$\n",
    "We can use that to compare the kernel Gram matrix $K$ with the 'idealized' kernel made of the outer product of $y$ with itself:\n",
    "$$A(K, y) = \\frac{\\langle K, yy^T \\rangle}{\\sqrt{\\langle K, K \\rangle \\langle yy^T, yy^T \\rangle}} = \\frac{\\langle K, yy^T \\rangle}{M \\sqrt{\\langle K, K \\rangle}}.$$\n",
    "\n",
    "The Kernel-Target alignment give us an approximate indicator of the quality of our kernel on our specific problem. the uncentered kernel\n",
    "alignment of Cristianini et al. (2001) does not correlate well with performance and thus, in general,\n",
    "cannot be used effectively for learning kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "ef305c4d-418a-4d13-8c40-55a1cfb35eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_target_alignment(K, y):\n",
    "    Y = np.outer(y, y)\n",
    "    norm = np.sqrt(np.sum(K * K)) * y.shape[0]\n",
    "    return np.sum(K * Y) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc589bf-035e-4a5b-b8c1-f85ea3a7fc53",
   "metadata": {},
   "source": [
    "#### Centered Kernel-Target alignment\n",
    "\n",
    "\n",
    "The Centered Kernel-Target alignment has been proposed in [2], and is computed as in (https://digitalcommons.uri.edu/cgi/viewcontent.cgi?article=2106&context=oa_diss).\n",
    "\n",
    "Centering a kernel is exremely important\n",
    "\n",
    "$$\\mathrm{center}(K) = U K U^T, U = \\Big[I - \\frac{11^T}{M}\\Big]$$\n",
    "\n",
    "$$C(K, y) = A(\\mathrm{center}(K), yy^T)$$\n",
    "\n",
    "Can be used to guide an optimization process better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "dbe66d41-3aec-41e4-a0ad-2f9c8dc73f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_kernel(K):\n",
    "    m = Kc.shape[0]\n",
    "    U = np.eye(m) - (1/m) * np.outer([1] * m, [1] * m)\n",
    "    Kc = U @ K @ U.T\n",
    "    return Kc\n",
    "\n",
    "def centered_kernel_target_alignment(K, y):\n",
    "    return kernel_target_alignment(center_kernel(K), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97272160-db56-4c42-bfbf-1da4e9bd8dbe",
   "metadata": {},
   "source": [
    "### Task-model alignment\n",
    "\n",
    "We can rewrite each $g$ as a linear combination of the orthonormal functions $\\{ \\phi_\\rho \\}$, i.e.\n",
    "$$g(x) = \\sum_{\\rho=0}^\\infty a_\\rho \\phi_\\rho(x), \\qquad a_\\rho = \\int p(x) f(x) \\phi_\\rho(x) dx $$\n",
    "\n",
    "\n",
    "By setting $\\psi_\\rho = \\sqrt{\\eta_\\rho} \\phi_\\rho(x)$ we get an orthonormal basis. \n",
    "\n",
    "In the discrete case, the kernel is approximated by the matrix $K = [\\kappa(x_i, x_j)]$. Due to the symmetric nature of $\\kappa$, the matrix can be decomposed in $K = \\Phi \\Lambda \\Phi^T$ where $\\Lambda$ is a diagonal matrix of nonnegative real eigenvalues (in descending order) and $\\Phi$ is an unitary matrix where each column is the corresponding eigenvector. We can set $\\Psi = \\Phi \\Lambda^{1/2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620776ef-d532-4aaa-bb36-417a64df10ba",
   "metadata": {},
   "source": [
    "We can then express the target function $\\bar{f}$ and the predictor function $f^*$ in the form of $f(x) = \\sum_\\rho w_\\rho \\psi_\\rho(x)$. In the finite setting, we recover the weight vector $w = \\frac{1}{m} \\Lambda^{-1/2} \\Phi^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "78850f25-ac85-448c-af81-e187054839c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_coefficient(kernel_eigenvalues, kernel_eigenvectors, labels):\n",
    "    \"\"\"\n",
    "    Calculates the weights of a predictor given the labels and the kernel eigendecomposition,\n",
    "    as shown in (Canatar et al 2021, inline formula below equation 18).\n",
    "    :param kernel_eigenvalues: vectors of m nonnegative eigenvalues 'eta'\n",
    "    :param kernel_eigenvectors: vectors of m nonnegative eigenvectors 'phi'\n",
    "    :param labels: vector of m labels corresponding to 'm' ground truth labels or predictor outputs\n",
    "    :return: vector of m weights\n",
    "    \"\"\"\n",
    "    # get the number of training elements\n",
    "    m = kernel_eigenvalues.shape[0]\n",
    "\n",
    "    # invert nonzero eigenvalues\n",
    "    inv_eigenvalues = np.reciprocal(kernel_eigenvalues, where=kernel_eigenvalues > 0)\n",
    "\n",
    "    # weight vectors are calculated by inverting formula: y = \\sum_k=1^M w_k \\sqrt{eta_k} \\phi_k(x)\n",
    "    the_w = (1 / m) * np.diag(inv_eigenvalues ** 0.5) @ kernel_eigenvectors.T @ labels\n",
    "    the_w = (1 / m) * kernel_eigenvectors.T @ labels\n",
    "    return the_w\n",
    "\n",
    "def calculate_weight_coefficient_wo_eigenvalues(kernel_eigenvectors, labels):\n",
    "    \"\"\"\n",
    "    Calculates the weights of a predictor given the labels and the kernel eigendecomposition,\n",
    "    as shown in (Canatar et al 2021, inline formula below equation 18).\n",
    "    :param kernel_eigenvalues: vectors of m nonnegative eigenvalues 'eta'\n",
    "    :param kernel_eigenvectors: vectors of m nonnegative eigenvectors 'phi'\n",
    "    :param labels: vector of m labels corresponding to 'm' ground truth labels or predictor outputs\n",
    "    :return: vector of m weights\n",
    "    \"\"\"\n",
    "    # get the number of training elements\n",
    "    m = kernel_eigenvectors.shape[0]\n",
    "\n",
    "    # weight vectors are calculated by inverting formula: y = \\sum_k=1^M w_k \\sqrt{eta_k} \\phi_k(x)\n",
    "    the_w = (1 / m) * kernel_eigenvectors.T @ labels\n",
    "    return the_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "f0535879-bf70-41de-971a-e05d8903f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_power_distribution(weight_coefficients, kernel_eigenvalues, rho):\n",
    "    powers = kernel_eigenvalues * (weight_coefficients ** 2)\n",
    "    return np.sum(powers[:rho]) / np.sum(powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f793363d-c6c3-4c9b-a330-4e7e45624126",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X[i] / np.linalg.norm(X[i]) for i in range(50)]\n",
    "K = build_kernel(lambda x, z: np.exp(-0.01 * np.linalg.norm(x-z)), X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "a30e9a39-434a-4667-be63-4ffabe5ee718",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, Phi = decompose_kernel(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "5c6e37a0-65f6-4cd1-ace9-97c632aba26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi[1].dot(Phi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "2d646e90-9f25-430e-a85e-e0954b5c396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = calculate_weight_coefficient(Lambda, Phi, y)\n",
    "w0 = calculate_weight_coefficient_wo_eigenvalues(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "b42b4570-397f-4abd-8b63-9cbb4229211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 - w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "daa64e4b-1d9a-41ec-a7a4-32282bc6ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000000\n",
      " 0.827579\n",
      " 0.832013\n",
      " 0.841189\n",
      " 0.852249\n",
      " 0.860618\n",
      " 0.860963\n",
      " 0.871843\n",
      " 0.873183\n",
      " 0.873595\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{cumulative_power_distribution(w, Lambda, i): 0.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fdcf5-3e35-4fbf-a3d3-7b11758fbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big learning\n",
    "https://arxiv.org/pdf/1302.4922.pdf\n",
    "https://arxiv.org/pdf/2210.11836.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
