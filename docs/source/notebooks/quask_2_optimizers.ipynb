{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a01c67-f360-4509-a39c-c58b17edb7db",
   "metadata": {},
   "source": [
    "# How to optimize a quantum kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238ab640-5ed8-4bbd-ae73-2d140d0f9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# to import quask, move from docs/source/notebooks to src\n",
    "sys.path.append('../../../src')\n",
    "import quask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fac109-b8ce-441e-82e7-3fe72289034b",
   "metadata": {},
   "source": [
    "One of the main challenges regarding quantum kernels is the need to choose the appropriate ansatz for each different problem. When little-to-no information about a certain task is known, we can use techniques to attempt to construct a suitable quantum kernel following an optimization problem. Two main approaches are possible:\n",
    "\n",
    "1. Choosing an ansatz where some of the parameters are the usual features, while others are freely tunable parameters that are optimized according to some cost function via a stochastic gradient descent-based algorithm.\n",
    "2. Avoid making any choice and let an optimization algorithm pick the entire quantum circuit.\n",
    "\n",
    "In this tutorial, we will demonstrate how to easily implement the first technique. Furthermore, we will also discuss how to leverage the _quask_ built-in features to implement the second technique. Finally, we will show how it is possible to efficiently achieve the best-performing linear combination of these kernels when dealing with multiple different kernels.\n",
    "\n",
    "## Optimization of quantum kernels in _quask_\n",
    "\n",
    "The package `quask.optimizer` allows defining an optimization procedure for quantum kernels. The main interface is the `BaseKernelOptimizer` class, which requires:\n",
    "* a kernel function, which will serve as the initial point of the optimization routine;\n",
    "* a kernel evaluator, which will be the cost function guiding the optimization;\n",
    "* possibly some input data, if needed by the kernel evaluator.\n",
    "\n",
    "Then, the `optimizer` method starts the optimization and will return a new instance of `quask.core.Kernel` to be used. You will use `BaseKernelOptimizer` directly if only to create a new optimization method, which can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392d3b3b-3c63-4603-8b4d-186ab42411cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "from quask.core import Ansatz, Kernel, KernelFactory, KernelType\n",
    "from quask.core_implementation import QiskitKernel\n",
    "from quask.optimizer.base_kernel_optimizer import BaseKernelOptimizer\n",
    "from quask.evaluator import CenteredKernelAlignmentEvaluator\n",
    "\n",
    "def create_qiskit_noiseless(ansatz: Ansatz, measurement: str, type: KernelType):\n",
    "    return QiskitKernel(ansatz, measurement, type, n_shots=None)\n",
    "\n",
    "KernelFactory.add_implementation('qiskit_noiseless', create_qiskit_noiseless)\n",
    "KernelFactory.set_current_implementation('qiskit_noiseless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2102b2ab-52d8-4ef4-a830-68b75f41dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomOptimizer(BaseKernelOptimizer):\n",
    "\n",
    "    def __init__(self, initial_kernel, X, y, ke):\n",
    "        super().__init__(initial_kernel, X, y, ke)\n",
    "\n",
    "    def optimize(self):\n",
    "        kernel = copy.deepcopy(self.initial_kernel)\n",
    "        cost = self.ke.evaluate(kernel, None, self.X, self.y)\n",
    "        N_TENTATIVES = 10\n",
    "        for i in range(N_TENTATIVES):\n",
    "            new_kernel = copy.deepcopy(kernel)\n",
    "            i_operation = np.random.randint(new_kernel.ansatz.n_operations)\n",
    "            i_feature = np.random.randint(new_kernel.ansatz.n_features)\n",
    "            i_wires = np.random.choice(range(new_kernel.ansatz.n_qubits), 2, replace=False).tolist()\n",
    "            i_gen = np.random.choice(['I', 'Z', 'X', 'Y'], 2, replace=True)\n",
    "            i_gen = \"\".join(i_gen.tolist())\n",
    "            i_bandwidth = np.random.rand()\n",
    "            new_kernel.ansatz.change_operation(i_operation, i_feature, i_wires, i_gen, i_bandwidth)\n",
    "            new_cost = self.ke.evaluate(new_kernel, None, self.X, self.y)\n",
    "            print(\"Cost of the new solution:\", new_cost)\n",
    "            if cost > new_cost:\n",
    "                kernel = new_kernel\n",
    "                cost = new_cost\n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f40424-7273-4dfc-804d-31260cffc6da",
   "metadata": {},
   "source": [
    "Let's unpack the content of this function. The initialization is identical to the one in `BaseKernelOptimizer`, as we don't really need any new parameters (we may have added `N_TENTATIVES`, but for the sake of simplicity, we can keep it as is).\n",
    "\n",
    "The `optimize` function effectively starts from the initial kernel and proceeds iteratively N_TENTATIVES times by changing a single operation within the quantum circuit with a completely random operation. When the result improves, indicating a lower cost for the kernel evaluator, the new solution is accepted. Clearly, this is a rather inefficient way to optimize the quantum kernel, and more sophisticated techniques are shown below.\n",
    "\n",
    "We can test this approach in a simple context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d89229-f85e-4b14-82f3-403a794442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 4\n",
    "N_OPERATIONS = 5\n",
    "N_QUBITS = 4\n",
    "ansatz = Ansatz(n_features=N_FEATURES, n_qubits=N_QUBITS, n_operations=N_OPERATIONS)\n",
    "ansatz.initialize_to_identity()\n",
    "kernel = KernelFactory.create_kernel(ansatz, \"Z\" * N_QUBITS, KernelType.FIDELITY)\n",
    "\n",
    "N_ELEMENTS_PER_CLASS = 20\n",
    "iris = load_iris()\n",
    "X = np.row_stack([iris.data[0:N_ELEMENTS_PER_CLASS], iris.data[50:50+N_ELEMENTS_PER_CLASS]])\n",
    "y = np.array([0] * N_ELEMENTS_PER_CLASS + [1] * N_ELEMENTS_PER_CLASS)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5454)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "ce = CenteredKernelAlignmentEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd5b364-c21f-4fb3-83da-10ce32be7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: -0.42299755542166695\n",
      "Cost of the new solution: -0.28109278237003543\n",
      "Cost of the new solution: -0.0961605171721972\n",
      "Cost of the new solution: -0.27011666753748764\n",
      "Cost of the new solution: -0.03337677439165289\n",
      "Cost of the new solution: -0.2771358170950197\n",
      "Cost of the new solution: -0.15948562200769945\n",
      "Cost of the new solution: -0.07431221779515468\n",
      "Cost of the new solution: -0.2751971110940814\n",
      "Cost of the new solution: -0.27668879041145483\n",
      "Cost of the new solution: -0.03265594563162022\n",
      "The final cost is: -0.42299755542166695\n"
     ]
    }
   ],
   "source": [
    "print(\"The initial cost is:\", ce.evaluate(kernel, None, X_train, y_train))\n",
    "optimizer = RandomOptimizer(kernel, X_train, y_train, ce)\n",
    "optimized_kernel = optimizer.optimize()\n",
    "print(\"The final cost is:\", ce.evaluate(optimized_kernel, None, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583cc32-6d5a-4c0d-be1b-3ea36152c361",
   "metadata": {},
   "source": [
    "The result of the optimization can be used exactly like any other kernel object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac08083-2ce0-4b74-9f6d-9736a9bf75ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='precomputed')\n",
    "K_train = optimized_kernel.build_kernel(X_train, X_train)\n",
    "model.fit(K_train, y_train)\n",
    "K_test = optimized_kernel.build_kernel(X_test, X_train)\n",
    "y_pred = model.predict(K_test)\n",
    "accuracy = np.sum(y_test == y_pred) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4cd0d-49ac-4aec-8c9e-c665a3127d3a",
   "metadata": {},
   "source": [
    "## Combinatorial optimization of a quantum kernel\n",
    "\n",
    "A plethora of techniques has been implemented in _quask_ and can be used in different contexts according to the available computational resources and the volume of data to be analyzed. The functioning of these algorithms is detailed in [inc23].\n",
    "\n",
    "### Bayesian optimizer\n",
    "\n",
    "Bayesian optimization is the simplest and usually the most effective technique to use. It is known to work best for low-dimensional problems where the function to optimize is a black box costly to evaluate, which is often the case in our context (although the optimization might not be so low-dimensional). This approach is based on the library [scikit-optimize](https://scikit-optimize.github.io/stable/), which needs to be installed separately from _quask_ via the command ``pip install scikit-optimize``.\n",
    "\n",
    "Note that a KeyError '' might occur at this point if you have not configured a default backend for _quask_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dcbf0be-809d-4d25-8c1b-02a37c402e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quask.optimizer.bayesian_optimizer import BayesianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b2e5257-be24-4d38-9708-41a4491d126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: -0.42299755542166695\n",
      "Epoch of training i=0\n",
      "Epoch of training i=1\n",
      "The final cost is: -0.22919895370100746\n"
     ]
    }
   ],
   "source": [
    "print(\"The initial cost is:\", ce.evaluate(kernel, None, X_train, y_train))\n",
    "optimizer = BayesianOptimizer(kernel, X_train, y_train, ce)\n",
    "optimized_kernel = optimizer.optimize(n_epochs=2, n_points=1, n_jobs=1)\n",
    "print(\"The final cost is:\", ce.evaluate(optimized_kernel, None, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f0c7e-ed84-40f1-b7cd-3630f3d19945",
   "metadata": {},
   "source": [
    "### Meta-heuristic optimizer\n",
    "\n",
    "At the moment we only support Particle Swarm but we plan to support other techniques, such as evolutionary (genetic) algorithms. This approach is based on the library [opytimizer](https://opytimizer.readthedocs.io/en/latest/), which needs to be installed separately from _quask_ via the command ``pip install opytimizer``.\n",
    "\n",
    "Due to the extremely high computational cost, you can only use this technique for the smallest circuits (<2 operations, <2 qubits); in general is better to rely on the other techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4643f83b-23e3-4464-8612-17a6ed007031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quask.optimizer.metaheuristic_optimizer import MetaheuristicOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ad112-4c78-416f-a336-f01e62f4f29a",
   "metadata": {},
   "source": [
    "### Greedy optimizer\n",
    "\n",
    "The greedy optimization tries any possible value for the first operation, chooses the best one, and proceeds with the following operations in a sequential fashion. Despite its simplicity, it is quite an expensive technique. This approach is based on the library [mushroom-rl](https://mushroomrl.readthedocs.io/en/latest/), which needs to be installed separately from _quask_ via the command ``pip install mushroom_rl``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb2de0-4782-49b8-bc68-c81c3a7e5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quask.optimizer.greedy_optimizer import GreedyOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120aa17-11ae-48d5-b076-b24a5d82f229",
   "metadata": {},
   "source": [
    "### Reinforcement learning optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f44fd3-5ad6-42fa-8e2c-d9984ba317c2",
   "metadata": {},
   "source": [
    "Optimizes the quantum kernel by setting up a reinforcement learning environment and using SARSA Lambda algorithm. This approach is based on the library [mushroom-rl](https://mushroomrl.readthedocs.io/en/latest/), which needs to be installed separately from _quask_ via the command `pip install mushroom_rl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e8152-e01e-4cae-a118-681556357902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quask.optimizer.reinforcement_learning_optimizer import ReinforcementLearningOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a38f64-bdf9-4ad3-b49a-2204187d7d0a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[llo20] Lloyd, S., Schuld, M., Ijaz, A., Izaac, J., & Killoran, N. (2020). Quantum embeddings for machine learning. arXiv preprint arXiv:2001.03622.\n",
    "\n",
    "[inc23] Incudini, M., Lizzio Bosco, D., Martini, F., Grossi, M., Serra, G., and Di Pierro, A., \"Automatic and effective discovery of quantum kernels\", arXiv e-prints, 2022. doi:10.48550/arXiv.2209.11144."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d91993e-0cba-43fd-a0d0-1a795cda8769",
   "metadata": {},
   "source": [
    ".. note::\n",
    "\n",
    "    Author's note"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
