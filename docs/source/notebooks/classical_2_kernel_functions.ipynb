{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ab8876-f9b6-42af-ac7f-70fca2336dba",
   "metadata": {},
   "source": [
    "# Kernel machines\r\n",
    "\r\n",
    "In the [previous tutorial](classical_1_linear_to_kernel.html), we introduced linear models for regression. These models are effective under certain assumptions, primarily that the target function to be learned is linear. In this tutorial, we'll explore how to adapt Ridge regression models to capture more complex relationships within the data while maintaining efficient trainin.\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51d43c84-2479-4f35-a589-cc83eb1698cd",
   "metadata": {},
   "source": [
    ".. note ::\n",
    "\n",
    "    The ['Data Mining Book'](https://dataminingbook.info/book_html/) provides excellent material that can be consulted online for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523c8d9-a2e9-478b-b18b-5e690af1f94f",
   "metadata": {},
   "source": [
    "## Feature map and kernel functions\n",
    "\n",
    "Let $\\mathbf{x}$ belong to the input space $\\mathcal{X} = \\mathbb{R}^d$. A _feature map_, denoted as $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$, is a transformation of input attributes into a Hilbert space $\\mathcal{H}$. This mapping allows us to represent the original features in a richer way, allowing us to solve the learning problem more effectively.\n",
    "\n",
    "Consider a scenario where our target function is given by $f(\\mathbf{x}) = v_0 \\mathbf{x}_1^2 + v_1 \\mathbf{x}_1 \\mathbf{x}_2 + v_2 \\mathbf{x}^2$. This target function exhibits quadratic relationships within the features, making it unsuitable for learning using a simple linear regressor in the form $\\tilde{f}(\\mathbf{x}) = w_1 \\mathbf{x}_1 + w_2 \\mathbf{x}_2$. To address this issue, we can define a feature map:\n",
    "\n",
    "$$\\phi(\\mathbf{x}) = \\left[\\begin{array}{c} \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_1 \\mathbf{x}_2 & \\mathbf{x}_1^2 &  \\mathbf{x}_2^2 \\end{array}\\right]^\\top$$\n",
    "\n",
    "and then use a linear regressor that operates directly on the transformed vector, $\\tilde{f} = \\sum_{j=1}^5 w_j \\phi(\\mathbf{x})_j$. In situations where the dual form of the linear (Ridge) regressor is used, we can directly replace the Euclidean inner product $\\langle \\cdot, \\cdot \\rangle$ with the _kernel function_:\n",
    "\n",
    "$$\\kappa(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle_\\mathcal{H}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f61d7b-b93a-49ee-8de9-6c238dda4890",
   "metadata": {},
   "source": [
    "### Calculating the kernel can be more convenient than calculating the explicit representation of the feature map\n",
    "\n",
    "A major advantage of using a kernel function is the efficiency it offers in calculating complex relationships without explicitly representing $\\phi(\\mathbf{x})$. This is particularly evident in cases like the polynomial kernel, designed to capture polynomial relationships of arbitrary degrees within the data:\n",
    "$$\\kappa(\\mathbf{x}, \\mathbf{x}') = (\\langle \\mathbf{x}, \\mathbf{x}' \\rangle + b)^c$$\n",
    "Its feature map has $k = \\sum_{j = 1}^c \\binom{c}{j}$ components, $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^k$, with $k \\gg d$. In many cases, the Hilbert space of the feature map has a much higher dimensionality than the original input space.\n",
    "\n",
    "A more striking example is the Gaussian or RBF kernel:\n",
    "$$\\kappa(\\mathbf{x}, \\mathbf{x}') = \\exp(-c \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert_2^2)$$\n",
    "Its calculation is straightforward, but the underlying feature map transforms $\\mathbf{x} \\in \\mathbb{R}^d$ into a Gaussian function with a mean value in $\\mathbf{x}$ itself, $\\phi(\\mathbf{x}) \\in L_2(\\mathbb{R}^d)$. In this case, $\\mathcal{H} = L_2(\\mathbb{R}^d)$ is infinite-dimensional, and constructing an explicit representation is unfeasible. A naive attempt would require value discretization and truncation to finite intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb217a77-cc1c-4f0c-8fb2-8061dd3d055b",
   "metadata": {},
   "source": [
    "### Positive semidefiniteness of kernel functions\n",
    "\n",
    "The most important fact about a kernel function is that it represents an inner product in some Hilbert space. To determine if a bilinear form is a valid kernel, we only need to ensure that it behaves like an inner product, which means it has to be positive semidefinite. This latter property also implies that the function is symmetric.\n",
    "\n",
    "If we can prove that $\\kappa$ is a positive semidefinite bilinear form, then there exists a (non-unique) Hilbert space $\\mathcal{H}$ and a feature map $\\phi$ satisfying $\\kappa(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_\\mathcal{H}$. This is true even if we don't know their exact definitions or how to compute them explicitly. Furthermore, positive semidefiniteness implies that, given data $\\mathbf{x}^1, ..., \\mathbf{x}^m$, the kernel Gram matrix $K_{i,j} = \\kappa(\\mathbf{x}^i, \\mathbf{x}^j)$ is positive semidefinite.\n",
    "\n",
    "Conversely, it is trivially true that if we define a kernel explicitly from the feature map, positive semidefiniteness holds by construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d4dd6-7b26-42e2-b517-0f6c06c9f593",
   "metadata": {},
   "source": [
    "### Reproducing Kernel Hilbert Space\n",
    "\n",
    "Let $\\kappa$ be a kernel function. We have infinitely many feature maps and Hilbert spaces corresponding to the given kernel. However, there is a unique _reproducing kernel feature map_ $\\Phi_x : \\mathcal{X} \\to \\mathbb{R}$, defined as $\\Phi_x = \\kappa(\\cdot, \\mathbf{x})$. We can use such a feature map to define the following pre-Hilbert vector space:\n",
    "\n",
    "$$ \\mathcal{V} = \\mathrm{span}\\{ \\Phi_x \\mid x \\in \\mathcal{X} \\} = \\left\\{f(\\cdot) = \\sum_{i=1}^n \\alpha_i \\kappa(\\cdot, x^i) \\mid n \\in \\mathbb{N}, x^i \\in \\mathcal{X} \\right\\}. $$\n",
    "\n",
    "We can prove that the following function is an inner product on $\\mathcal{V}$, which means it is symmetric, bilinear, and positive semidefinite:\n",
    "\n",
    "$$ \\langle f, g \\rangle \n",
    "= \\left\\langle \\sum_{i} \\alpha_i \\kappa(\\cdot, x^i),  \\sum_{j} \\beta_j \\kappa(\\cdot, x^j) \\right\\rangle\n",
    "= \\sum_{i, j} \\alpha_i \\beta_j \\kappa(x^i, x^j).$$\n",
    "\n",
    "We can also prove that $\\kappa$ has the reproducing property, which means that the following equation holds:\n",
    "\n",
    "$$\\langle f, \\kappa(\\cdot, x^j)\\rangle = f(x^j).$$\n",
    "\n",
    "We can define the vector space $\\mathcal{H} = \\overline{\\mathcal{V}}$, which is complete and, thus, a Hilbert space. This latter one is denoted as the Reproducing Kernel Hilbert Space (RKHS) of $\\kappa$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ae49f-2496-4c41-bc49-cd324397df9f",
   "metadata": {},
   "source": [
    "## Kernel Ridge regression\n",
    "\n",
    "We have defined the Ridge regressor in the previous tutorial, \n",
    "$$\\tilde{f}(\\mathbf{x}) = \\sum_{j=1}^m \\alpha_j \\langle \\mathbf{x}, \\mathbf{x}^j \\rangle$$\n",
    "where ${\\alpha} = (G + \\lambda I)^{-1} \\mathbf{y}$ is the solution of the optimization problem expressed via the Lagrangian multipliers, and $G = X^\\top X$ Gram matrix. To define a _kernel_ Ridge regressor we just have to substitute the inner product with the kernel function, and the Gram matrix $G$ with the _kernel_ Gram matrix $K_{i,j} = \\kappa(x^i, x^j)$, \n",
    "$$\\tilde{f}(\\mathbf{x}) = \\sum_{j=1}^m \\alpha_j \\kappa(\\mathbf{x}, \\mathbf{x}^j).$$\n",
    "We can easily test the model on a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2547b3c0-499e-4d41-bc49-924eec36bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m, d = 100, 2\n",
    "unknown_f = lambda x: 3.2 * x[0] * x[1] + 5.2 * x[0]**3\n",
    "\n",
    "X = np.random.random(size=(m, d))\n",
    "y = np.apply_along_axis(unknown_f, 1, X) + 0.33 * np.random.random(size=(m,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f828e-4172-456d-a74f-7a04e21ccf34",
   "metadata": {},
   "source": [
    "To test the approach, we need to define a kernel. Given the problem's structure, we choose a polynomial kernel of degree three. As this is a Ridge regression, the parameter `alpha` must be set, which corresponds to the strength of the regularization term.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e65ff4a0-6b8f-4109-b777-7a423bbb1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03978691374195227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "\n",
    "kernel_regressor = KernelRidge(alpha=0.1, kernel='polynomial', degree=3)\n",
    "kernel_regressor.fit(X_train, y_train)\n",
    "y_pred = kernel_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb0555-6880-4986-be3f-757d1dac6570",
   "metadata": {},
   "source": [
    "The kernel has been specified, in this example, in the arguments `kernel` and `degree`. Look at the documentation to see all the possibilities offere built-in in the _scikit-learn_ package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8613ac-3032-4d0e-9bb3-3a60981a0788",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "When we select a kernel that is not sophisticated enough to capture the inherent relationships within the dataset, we end up with a model that cannot effectively learn the target function. This phenomenon is known as _underfitting_, and it occurs when both the training set and testing set errors are high.\n",
    "\n",
    "In our example, this may occur if we use a linear kernel when the actual function is cubic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cccf314d-c0b3-4f25-8ffd-ec402a43a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error training: 0.3511775695752495\n",
      "Mean Squared Error testing: 0.5276843667598432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1234)\n",
    "\n",
    "kernel_regressor = KernelRidge(alpha=0.1, kernel='polynomial', degree=1)\n",
    "kernel_regressor.fit(X_train, y_train)\n",
    "y_pred_train = kernel_regressor.predict(X_train)\n",
    "y_pred_test = kernel_regressor.predict(X_test)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"Mean Squared Error training: {mse_train}\")\n",
    "print(f\"Mean Squared Error testing: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c43aa6-c60e-4170-a349-da2d0e1052e6",
   "metadata": {},
   "source": [
    "### Overfitting\r\n",
    "\r\n",
    "When we select a kernel that is too sophisticated, the model interpolates both the data and the noise within the dataset. This results in a model whose underlying function is extremely complicated and distant from the true target. This phenomenon is known as _overfitting_, and it occurs when the training set error is low, and the testing set error is high.\r\n",
    "\r\n",
    "In this case, setting a large regularization constant can mitigate the problem. A large regularization constant favors 'simple' solutions over complicated ones, even if they better interpolate the data.\r\n",
    "\r\n",
    "In our example, this may occur if we use a degree-50 kernel when the actual function is ubic:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a7b596b-5cf2-4f09-bd4c-5275739e2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning about singular matrices\n",
    "import warnings\n",
    "from scipy.linalg import LinAlgWarning\n",
    "warnings.filterwarnings(action='ignore', category=LinAlgWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d2c0aa3-b568-4237-816b-b587328367a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error training: 3.117245929034068e-05\n",
      "Mean Squared Error testing: 24185905.35320369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "kernel_regressor = KernelRidge(alpha=0.000001, kernel='polynomial', degree=50)\n",
    "kernel_regressor.fit(X_train, y_train)\n",
    "y_pred_train = kernel_regressor.predict(X_train)\n",
    "y_pred_test = kernel_regressor.predict(X_test)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"Mean Squared Error training: {mse_train}\")\n",
    "print(f\"Mean Squared Error testing: {mse_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
