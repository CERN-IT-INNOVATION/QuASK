{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ab8876-f9b6-42af-ac7f-70fca2336dba",
   "metadata": {},
   "source": [
    "# Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ae49f-2496-4c41-bc49-cd324397df9f",
   "metadata": {},
   "source": [
    "## Kernel ridge regression\n",
    "\n",
    "Now, we have seen that, when defining the regressor in term of the $\\alpha$ coefficients, the regressor is a linear combination of the inner product between the argument $\\mathbf{x}$ and the feature vector of the dataset $\\mathbf{x}^j$. \n",
    "\n",
    "This form leave open interesting possibilities. Instead of the inner product $\\langle x, x^j \\rangle$ in the Euclidean space $\\mathbb{R}^d$, we could transform our elements in some different representation $\\phi(\\mathbf{x})$, with $\\phi$ being a mapping from $\\mathbb{R}^d$ to some Hilbert space $\\mathcal{H}$ and named _feature map_, and substitute the Euclidean inner product with\n",
    "$$ \\langle \\phi(x), \\phi(x^j) \\rangle_\\mathcal{H}.$$\n",
    "If we decide to use $\\phi(x)$ instead of $\\mathbf{x}$ in the ridge classifier, the regressor will not be linear (with respect to the feature space $\\mathbb{R}^d$) anymore, allowing to capture more intricate relationship within the data. The function\n",
    "$$\\kappa(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_\\mathcal{H}.$$\n",
    "is called a _kernel_, and a function in the form\n",
    "$$\\tilde{f}(x) = \\sum_{j=1}^m \\alpha_j \\kappa(x, x^j)$$\n",
    "is a _kernel machine_. \n",
    "\n",
    "\n",
    "### Comparison between primal and dual form of the kernel ridge regressor\n",
    "\n",
    "Suppose we can efficiently compute the feature map $\\phi : \\mathbb{R}^d \\to \\mathcal{H}$. By mapping any feature vector $\\mathbf{x}$ to its enhanced representation $\\phi(\\mathbf{x})$, we obtain a regressor in the following form:\n",
    "\n",
    "$$\\tilde{f}(\\mathbf{x}) = \\sum_{j = 1}^{\\dim \\mathcal{H}} w_j \\phi_j(x)$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\tilde{w} = (\\Phi(X)^\\top \\Phi(X) + \\lambda I)^{-1} \\Phi(X)^\\top \\mathbf{y}.$$\n",
    "\n",
    "We can easily obtain the dual form from its primal by explicitly constructing the kernel function as the inner product of its transformed vectors. The regressor takes the form:\n",
    "\n",
    "$$\\tilde{f}(\\mathbf{x}) = \\sum_{j = 1}^{m} \\alpha_j \\langle \\phi(x), \\phi(x^j) \\rangle$$\n",
    "\n",
    "However, it's worth noting that we may have access to the kernel function $\\kappa$ and can efficiently compute its values, even if we don't have access to the corresponding feature map. This is the case, for example, with the Gaussian kernel:\n",
    "\n",
    "$$\\kappa(x, x') = \\exp(-c \\lVert x - x' \\rVert)$$\n",
    "\n",
    "with $c > 0$. The corresponding feature map would map $x \\in \\mathbb{R}^d$ to a Gaussian function in the space $L_2(\\mathbb{R}^d)$ with a mean value in $x$ and fixed variance. The situation for which we can compute efficiently $\\kappa$ but not $\\phi$ is named _kernel trick_. \n",
    "\n",
    "Creating an exact primal form for a kernel machine that uses this kernel is challenging. However, we can try to construct a feature map $\\bar{\\phi}$ such that $\\bar{\\phi}$ is efficiently computable and $\\langle \\bar{\\phi}(x), \\bar{\\phi}(x') \\rangle \\approx \\kappa(x, x')$. Many techniques have been proposed to build such an approximation, one of them being the Random Fourier Feature \\[rff08\\]. In this case, the feature map is approximated with an arbitrary number of random projections. The more projections used, the better the approximation will be.\n",
    "\n",
    "How can we choose whether to use the primal or dual form of the kernel regressor? Here are some guidelines:\n",
    "\n",
    "- When the feature map is easy to compute, use the primal form.\n",
    "- When the feature map is hard to compute, but the dataset size $m$ is too high, you can either approximate the kernel function and use the primal form or subsample the dataset and use the dual form.\n",
    "- When the feature map is hard to compute, and the dataset size $m$ is modest, use the dual form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677983f-c537-4c00-a4b8-f38885058e4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2f21f-f017-49d7-a5d9-77a98c066cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
