{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd79fd9a-1618-4c9b-a910-b95ce5e812dd",
   "metadata": {},
   "source": [
    "# Linear and Ridge regression\n",
    "\n",
    "Our first tutorial delves into the motivation behind the application of kernel methods. \n",
    "We begin by demonstrating regression analysis using the simplest model, the linear regressor. \n",
    "Next, we explore how Ridge regularization can be applied to mitigate numerical errors during model training. \n",
    "Finally, we delve into solving these learning problems using the Lagrangian multiplier methods, highlighting how this method gives rise to the primal and dual forms of the Ridge regressor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e9fef-3ef9-4a35-8b4f-e03bb7efdd2c",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is the simplest machine learning model used for learning a target function, denoted as $f$. It requires a dataset of samples, $\\{ (\\mathbf{x}^j, y^j) \\}_{j=1}^m$. Each feature vector is represented as $\\mathbf{x}^j \\in \\mathbb{R}^d$, and has been sampled i.i.d. from some unknown probability distribution $p(\\mathbf{x})$. The labels are defined as $y^j = f(\\mathbf{x}^j) + \\varepsilon^i$, with each $\\varepsilon^i$ representing random Gaussian noise with zero mean and fixed variance.\n",
    "\n",
    "Let $X = \\left[ \\begin{array}{c} (\\mathbf{x}^1)^\\top \\\\ \\vdots \\\\ (\\mathbf{x}^m)^\\top \\end{array}\\right] \\in \\mathbb{R}^{m \\times d}$, which is the design matrix, and $\\mathbf{y} = \\left[ \\begin{array}{c} y^1 \\\\ \\vdots \\\\ y^m  \\end{array}\\right] \\in \\mathbb{R}^{m \\times 1}$, representing the regressand.\n",
    "\n",
    "We can generate randomly a synthetic dataset for our demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7eaf452-ed1d-41c4-ad13-5f51009c2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensionality    : design matrix X.shape=(100, 2) | regressand y.shape=(100,)\n",
      "Example of feature vector : [0.57980451 0.18366092]\n",
      "Example of label          : 0.18893031545613453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set the dimensionality of the problem: number of samples of the dataset, dimensionality of the feature vector\n",
    "m, d = 100, 2\n",
    "\n",
    "# create a target function f to learn, according to our assumptions it should be a linear function\n",
    "unknown_w = np.random.random(size=(d,))\n",
    "unknown_f = lambda x: unknown_w.dot(x)\n",
    "\n",
    "# generate the synthetic dataset: first the features...\n",
    "X = np.random.random(size=(m, d))\n",
    "\n",
    "# generate the synthetic dataset: ... and then the noisy labels\n",
    "noiseless_y = np.apply_along_axis(unknown_f, 1, X)\n",
    "noise = 0.1 * np.random.random(size=(m,))\n",
    "y = noiseless_y + noise\n",
    "\n",
    "print(f\"Dataset dimensionality    : design matrix {X.shape=} | regressand {y.shape=}\")\n",
    "print(f\"Example of feature vector : {X[0]}\")\n",
    "print(f\"Example of label          : {y[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab6445-f1e6-4ae2-b358-79b3a9a4523f",
   "metadata": {},
   "source": [
    "We are going to build a function:\n",
    "\n",
    "$$\\tilde{f}(\\mathbf{x}) = \\langle \\mathbf{x}, \\mathbf{w}\\rangle + b$$\n",
    "\n",
    "such that $\\tilde{f}$ is as close as possible to $f$. Note that we will omit the bias $b$ \\[foot1\\]. The ideal scenario would be to find the $\\tilde{f}$, or the vector of weights $\\tilde{\\mathbf{w}}$, that minimizes the _expected risk_:\n",
    "\n",
    "$$\\tilde{f}(\\mathbf{x}) = \\arg\\min_h \\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})}\\left[(h(\\mathbf{x}) - f(y))^2\\right]$$\n",
    "\n",
    "However, we have no access to $f$ other than the given dataset, so we can only minimize the _empirical risk_:\n",
    "\n",
    "$$\\tilde{f}(\\mathbf{x}) = \\arg\\min_h \\sum_{j=1}^m (h(\\mathbf{x}^j) - y^j)^2$$\n",
    "$$\\tilde{w} = \\arg\\min_{w \\in \\mathbb{R}^d} (\\langle \\mathbf{x}^j, \\mathbf{w}\\rangle - y^j)^2$$\n",
    "\n",
    "Provided that the columns of $X$ are linearly independent, the problem has a unique, analytical solution obtained by setting the derivative of the objective function to zero, \n",
    "$$\\begin{array}{rl} \n",
    "\\nabla_w \\left[\\lVert X w - y \\rVert\\right] & = 0 \\\\\n",
    "\\nabla_w \\left[(Xw)^\\top Xw-(Xw)^\\top y - y^\\top(Xw)+y^\\top y\\right] & = 0 \\\\\n",
    "\\nabla_w \\left[w^\\top X^\\top X w-2(Xw)^\\top y + y^\\top y\\right] & = 0 \\\\\n",
    "2 X^\\top X w - 2 X^\\top y & = 0 \\\\\n",
    "X^\\top X w & = X^\\top y \n",
    "\\end{array}$$\n",
    "which results in the following solution, \n",
    "$$\\tilde{w} = (X^\\top X)^{-1} X^\\top \\mathbf{y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b254a1dc-5f8f-439d-aaf2-f96aafaf2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical error between target and estimated functions: 0.35801910246331653\n",
      "True error between target and estimated functions: 0.06560906515727047\n"
     ]
    }
   ],
   "source": [
    "# standard matrix inversion, not numerically stable\n",
    "G = np.linalg.inv(X.transpose().dot(X))\n",
    "\n",
    "# more stable way to calculate matrix inversion\n",
    "G = np.linalg.solve(X.transpose().dot(X), np.eye(2))\n",
    "\n",
    "# finish calculating\n",
    "estimated_w = G.dot(X.transpose()).dot(y)\n",
    "\n",
    "# the empirical risk is not zero because of the noise in the dataset label\n",
    "print(f\"Empirical risk between target and estimated functions: {np.linalg.norm(np.apply_along_axis(lambda x: estimated_w.dot(x), 1, X) - y)}\")\n",
    "\n",
    "# nonetheless, we get decent generalization error\n",
    "print(f\"True risk between target and estimated functions: {np.linalg.norm(estimated_w - unknown_w)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "270de918-4c2e-40ff-8224-b2729e5647af",
   "metadata": {},
   "source": [
    ".. warning ::\n",
    "    When using linear regression, we are making certain assumptions, such as assuming that the target function is linear, the data has been independently and identically distributed (i.i.d.) from some distribution, and the noise's variance is constant. If the target function does _not_ adhere to these assumptions, it will be challenging to solve the learning task with a linear classifier, and we should consider choosing a different, more suitable machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bcc6f-dd10-4771-8cbd-7830a7036558",
   "metadata": {},
   "source": [
    "Using the capabilities of the _scikit-learn_ framework instead of writing everything from scratch with NumPy is much easier and less error-prone. The same example demonstrated earlier can be rephrased as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bc523cc-f7c9-4731-88f5-785bdab7ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights with the LinearRegressor class: [0.19503256 0.14576726]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create model\n",
    "lin_reg = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# training\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# retrieve the weight parameters\n",
    "another_estimated_w = lin_reg.coef_\n",
    "print(\"Weights with the LinearRegressor class:\", another_estimated_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972f42a-3280-4447-bebd-eb2ad20d1604",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "In solving the learning problem above, numerical errors can arise when computing the inverse of $X^\\top X$, especially when it has nearly singular values. To address this issue, we can introduce positive elements on the principal diagonal. This adjustment reduces the condition number and eases matrix inversion. In this case, we aim to solve the following problem:\n",
    "\n",
    "$$\\tilde{w} = \\arg\\min_{w \\in \\mathbf{R}^d} \\sum_{j=1}^m (\\langle \\mathbf{x}^j, \\mathbf{w}\\rangle - y^j)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the regularization term.\n",
    "\n",
    "A regressor whose associated learning problem is the one described above is called a *ridge regressor*. The solution can still be found analytically:\n",
    "\n",
    "$$\\tilde{w} = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de9483f2-91af-40ee-80f3-0ab3a6888e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights with the Ridge class: [0.15521267 0.13250445]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "rigde_reg = Ridge(alpha=10.0, fit_intercept=False)\n",
    "rigde_reg.fit(X, y)\n",
    "ridge_estimated_w = rigde_reg.coef_\n",
    "print(\"Weights with the Ridge class:\", ridge_estimated_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695aa8de-a8e2-49cc-adff-c6f2e412bfa1",
   "metadata": {},
   "source": [
    "## The Lagrangian multiplier method\n",
    "\n",
    "The solution to the _ridge regularization_ problem can be found via the Lagrangian multipliers methods. It is applied with problems in the following form:\n",
    "$$\\begin{array}{rl} \\min_w & f(w) \\\\ \\text{constrained to} & h_j(w) = 0, \\text{with }j = 1, ..., k\\end{array}$$\n",
    "We assume that both $f$ and any $h_j$ is convex. The Lagrangian function is defined as\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\mathbf{\\alpha}) = f(\\mathbf{w}) + \\sum_{j=1}^k \\mathbf{\\alpha}_j h_j(\\mathbf{w})$$\n",
    "and the parameters $\\mathbf{\\alpha}_j$ are called Lagrangian multipliers. The solution of this optimization problem can be found by setting the partial derivatives to zero, \n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\alpha}} = 0$\n",
    "and solve for $\\mathbf{w}$ and $\\mathbf{\\alpha}$. \n",
    "\n",
    "In this case, we can solve two possible optimization problems, \n",
    "$$\\min_w \\max_\\alpha \\mathcal{L}(w, \\alpha)$$\n",
    "which is the _primal_ form of the optimization problem, or\n",
    "$$\\max_\\alpha \\min_w \\mathcal{L}(w, \\alpha)$$\n",
    "which is the _dual_ form. In general the solution of the two opimization problems are different, but, under some assumptions (the KKT conditions) they coincide. These assutptions hold for the optimization problem underlying the Rigde regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86430efb-312a-4a71-b115-c9a5c3d4382a",
   "metadata": {},
   "source": [
    "### Solving the Ridge regression with Lagrangian multipliers\n",
    "\n",
    "When considering the following objective function,\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\sum_{j=1}^m (\\mathbf{w}^\\top \\mathbf{x}^j - y^j)^2 + \\lambda \\langle \\mathbf{w}, \\mathbf{w}\\rangle,$$\n",
    "the optimization problem $\\arg\\min_w \\mathcal{L}(w)$ is unconstrained, and it is solved analytically without the Lagrangian multiplier method.\n",
    "\n",
    "We can, although, convert this problem into a constraint optimization. It seems that we are making things more difficult, but truly this way will reveal important insights. We introduce the variables $\\mathbf{z}_j = \\mathbf{w}^\\top \\mathbf{x}^j - y^j$, and this variable can be associated with the constraint $h_j(\\mathbf{z}, \\mathbf{w}) \\equiv \\mathbf{w}^\\top \\mathbf{x}^j - y^j - \\mathbf{z}_j = 0$. The optimization problem becomes\n",
    "$$\\begin{array}{rl} \n",
    "\\arg\\min_w & \\langle z, z \\rangle + \\lambda \\langle w, w \\rangle \\\\ \n",
    "\\text{constrained to} & h_j(w) = 0, \\text{with }j = 1, ..., m\n",
    "\\end{array}$$\n",
    "For each constraint, we define a Lagrangian multiplier $\\alpha_j$. The associated Lagrangian is:\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\mathbf{z}, \\mathbf{\\alpha}) = \\langle z, z \\rangle + \\lambda \\langle \\mathbf{w}, \\mathbf{w}\\rangle + \\sum_{j=1}^m \\alpha_j (x^j w -y^j -z^j).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae52d7-8145-4c5e-90b7-26792b7ba599",
   "metadata": {},
   "source": [
    "Solving this problem in its primal form leads to the solution that we have already found. However, if we solve the dual form, \n",
    "$$\\max_\\alpha \\min_{w, z} \\mathcal{L}(\\mathbf{w}, \\mathbf{z}, \\mathbf{\\alpha}),$$\n",
    "we obtain that\n",
    "$$\\begin{array}{rl} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_j} = 0 & \\implies \\mathbf{z}_j = \\mathbf{\\alpha}_j, \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_j} = 0 & \\implies \\mathbf{w}_j = - \\frac{1}{\\lambda} \\sum_{k=1}^m \\mathbf{\\alpha}_j x^j.\n",
    "\\end{array}$$\n",
    "This latter equation is especially important, as each weight of the model is expressed as a linear combination of the elements in the training set. Once substituted $w$ and $z$, the remaining objective takes the form:\n",
    "$$\\begin{array}{rl}\n",
    "\\mathcal{L}(\\mathbf{\\alpha}) & = \n",
    "\\langle \\alpha, \\alpha \\rangle\n",
    "+ \\lambda \\left\\langle - \\frac{1}{\\lambda} \\sum_{k=1}^m \\mathbf{\\alpha}_k x^k, - \\frac{1}{\\lambda} \\sum_{k=1}^m \\mathbf{\\alpha}_k x^k \\right\\rangle\n",
    "+ \\sum_{j=1}^m \\alpha_j \\left( - \\frac{1}{\\lambda} \\sum_{k=1}^m \\mathbf{\\alpha}_k x^k x^j - y^j - \\alpha^j\\right) \\\\\n",
    "& = - \\sum_{j=1}^m \\alpha_j^2 - \\frac{1}{\\lambda}\\sum_{j,k=1}^m \\alpha_j \\alpha_k \\langle x^j, x^k \\rangle - 2\\sum_{j=1}^m \\alpha_j y^j \\\\\n",
    "& = -\\langle \\alpha, \\alpha \\rangle - \\frac{1}{\\lambda} \\alpha^\\top G \\alpha - 2 \\langle \\alpha, y\\rangle\n",
    "\\end{array}$$\n",
    "where $G$ is the Gram matrix of the the inner products between the vectors $x^1, ..., x^m$, and corresponds to $G_{j,k} = \\langle x^j, x^k \\rangle$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fc46c-de89-434c-9dcc-87f14adacac6",
   "metadata": {},
   "source": [
    "Now, we have two ways of expressing the same Ridge regressor:\n",
    "* In the primal form, we have $\\tilde{f}(\\mathbf{x}) = \\sum_{j=1}^d \\mathbf{w}_j \\mathbf{x}_j$.\n",
    "* In the dual form, we have $\\tilde{f}(\\mathbf{x}) = \\sum_{j=1}^m \\alpha_j \\langle \\mathbf{x}, \\mathbf{x}^j \\rangle$. \n",
    "\n",
    "The primal form has the advantage that, in case we are dealing with a large amount of data ($m \\gg d$), the regressor is much more efficient. The dual form has the advantage of expressing the regressor in terms of its similarity to the elements of the training set, instead of on $\\mathbf{x}$ itself. This leads to new possibilities because by changing the notion of 'similarity,' we can create much more powerful models.ossibilities by redefining the concept of 'similarity.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e363e0-2e96-451f-bdba-ab690de855ca",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[foot1\\] as it can be incorporated as the weight vector $\\mathbf{w}$ via the equation $\\mathbf{w}^\\top \\mathbf{x} + b = [\\mathbf{w}, b]^\\top [\\mathbf{x}, 1] = (\\mathbf{w}')^\\top \\mathbf{x}'$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
